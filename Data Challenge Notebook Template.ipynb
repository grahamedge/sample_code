{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Croix Data Challenge - Graham Edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Outline\n",
    "\n",
    "The path from opening up this exploding data file to producing actionable insights was exciting, but complex. Here is a guide to structure the story.\n",
    "\n",
    "  1. [Exploratory Data Analysis](#chapter-1)\n",
    "      1. [Data Inspection](#chapter-1a)\n",
    "      2. [Dimensionality Reduction](#chapter-1b)\n",
    "  2. [Feature Engineering](#chapter-2)\n",
    "      1. [Correlations Between Variables](#chapter-2a)\n",
    "      2. [Explained Variances in PCA](#chapter-2b)\n",
    "  3. [Model Development](#chapter-3)\n",
    "      1. [Fitting Logistic Regression](#chapter-3a)\n",
    "      2. [Training and Test Accuracy](#chapter-3b)\n",
    "      3. [Accuracy vs. Precision: Confusion Matrix](#chapter-3c)\n",
    "  4. [Model Iteration](#chapter-4)\n",
    "      1. [Support Vector Machines](#chapter-4a)\n",
    "      2. [Deep Belief Network](#chapter-4b)\n",
    "      3. [Logistic Regression with Polynomial Feature Expansion](#chapter-4c)\n",
    "  5. [Conclusions and Insights](#chapter-5)\n",
    "      1. [Best Model](#chapter-5a)\n",
    "      2. [Insights](#chapter-5b)\n",
    "      3. [Next Steps](#chapter-5c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the Data <a id=\"chapter-1\"></a>\n",
    "Here is a little intro preamble for what will happen in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Inspecting the Data<a id=\"chapter-1a\"></a>\n",
    "Here we are going to inspect the data by examining things like:\n",
    "- the head of the dataframe\n",
    "- the column names and the data type for each column\n",
    "- whether there are missing values in any columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataframe = pd.read_csv('datafile.csv')\n",
    "\n",
    "#X = dataframe[features]\n",
    "#Y = dataframe[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a table describing the data we have\n",
    "\n",
    "| Variable | Description |\n",
    "| -- |:-- |\n",
    "| `X[0,:]` |This is a 0 or 1 feature to encode gender |\n",
    "| `X[1,:]`| This is the age feature |\n",
    "| `X[2,:]`| *This is the number of followers on Twitter* |\n",
    "\n",
    "Here I am summarizing some thoughts about the variable table above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction<a id=\"chapter-1b\"></a>\n",
    "Now we will see if there is a way to reduce the size of the data and eliminate strongly correlated features. We will try things like:\n",
    "- PCA\n",
    "- ?\n",
    "- ?\n",
    "\n",
    "Notice that the PCA doesn't actually do anything to the data when we execute the command `pca_machine.fit(X)` below - this only calculates the optimal feature combinations. We will need to use a command like `X_pca = pca_machine.transform(X)` later to produce the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import pca\n",
    "\n",
    "#pca_machine = pca(n_features = 5)\n",
    "#pca_machine.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrap-Up\n",
    "Now that we have taken a close look at the data, its time to try some feature engineering to produce more meaningful features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering <a id=\"chapter-2\"></a>\n",
    "Now I am discussing some things about feature engineering and how it is going to blow this data problem wide open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations Between Variables<a id=\"chapter-2a\"></a>\n",
    "We will want to combine together columns if they are strongly correlated with one another. We will detect correlated features by examining things like:\n",
    "- Pearson Correlation Matrix\n",
    "- ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some code to plot the correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explained Variances in PCA<a id=\"chapter-2b\"></a>\n",
    "To see what kinds of feature combinations would be more meaninful to describe the data, we will use Principal Component Analysis (PCA). Specifically we want to know how many principal components are needed to describe most of the data, so we will examing the explained variance for each additional component that we add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pca.fit(X)\n",
    "#print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Development <a id=\"chapter-3\"></a>\n",
    "Now that I have totally inspected the data, I'm going to try out some  models to accomplish the prediction that we are looking for. I will probably start with a simple model to learn something from the data, before possibly proceeding to more complicated models or features if needed.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "First we have to split the data into training and testing subsets.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.model import train_test_split\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression<a id=\"chapter-3a\"></a>\n",
    "We always try a logistic regression, since it is simple, fast, and interpretable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#logreg_model = LogisticRegression(C=1e5)\n",
    "#logreg_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy<a id=\"chapter-3b\"></a>\n",
    "Now after we have trained a model on a subset of the data, we need to check its accuracy on some withheld 'test data' in order to get an idea for how this model will generalize to data it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#code to calculate the accuracy on the training and test\n",
    "\n",
    "#Y_pred = logreg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix<a id=\"chapter-3c\"></a>\n",
    "Accuracy of the model only tells part of the story. Here we are going to look at the False Positives, True Positives, False Negatives, and True Negatives visually with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#confusion_matrix(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We just plotted confusion matrix but I'm not going to let it just stand there on its own. Let's talk a bit about False Positives, False Negatives, Precision, Accuracy, Recall, and all that stuff*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Iteration <a id=\"chapter-4\"></a>\n",
    "That logistic regression model was good, but we also saw that it seemed to be making (insert error here) types of errors. Now we will try out a few more complex models that should address this issue because of (insert cleverness here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines Model<a id=\"chapter-4a\"></a>\n",
    "Support Vector Machine (SVM) models are good at fitting nonlinear relationships between features, because they can implement a 'kernel trick' to transform the data into a higher-dimensional space in which some previously non-linear data become linearly separable.  Let's try this widely used model out on this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.svm import svc\n",
    "\n",
    "#svm_model = svc()\n",
    "#svm_model.fit(X, y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Belief Network<a id=\"chapter-4b\"></a>\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "You might iterate through to more and more complex models as you build the data story. ~~Probably~~ Definitely nobody should be using deep learning in a data challenge though!</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras import NO DONT DO IT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Feature Expansion<a id=\"chapter-4c\"></a>\n",
    "At this point we have tried some complex models, but we are thinking that maybe we should revisit some of the feature engineering from the beginning of the notebook. Let's try the simple logistic regression model again, but we will try generating new polynomial combinations of the original features to see if this improves the model accuracy.\n",
    "\n",
    "This will produce a lot of features to deal with, so we should perhaps try to prune down the features later once we find out the important ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions <a id=\"chapter-5\"></a>\n",
    "Now that we have examined the data, engineered features, and tried several models, it is time to ~~sleep~~ summarize what we have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model<a id=\"chapter-5a\"></a>\n",
    "We tried several models, some complicated and some simple. My recommendation is that the (MODEL) is the best choice for this problem for the following reasons:\n",
    "- simplicity?\n",
    "- overall accuracy?\n",
    "- best False Positive / False Negative rate?\n",
    "- fastest to train?\n",
    "- most interpretable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights<a id=\"chapter-5b\"></a>\n",
    "In the course of exploring and analyzing this data, some other points also became clear:\n",
    "- there are too many features, you should collect less data\n",
    "- there are not enough features, you should really get more data\n",
    "- this data is messy, I suggest that you collect it differently\n",
    "- there are a lot of missing values, you should try to fill in this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps and Recommendations<a id=\"chapter-5c\"></a>\n",
    "There are myriad ways to move forward with the results of this analysis, but here are a few that I think would provide the highest return-on-investment:\n",
    "1. Getting more data will help these machine learning models to generalize better\n",
    "2. Combining this data with (ADDITIONAL EXTERNAL DATA SOURCE) would bring in (INTERESTING NEW DIMENSION) and should allow you to (AMAZING NEW BUSINESS THING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=200 src='http://www.grahamedge.com/static/Graham_Edge_Square.jpg'>\n",
    "<div style=\"text-align: center;\" markdown=\"1\">** Hire me**</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Insight)",
   "language": "python",
   "name": "insight"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
